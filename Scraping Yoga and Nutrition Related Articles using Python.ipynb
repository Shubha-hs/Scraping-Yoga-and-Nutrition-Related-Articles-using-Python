{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Yoga and Nutrition Related Articles using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yoga** is an ancient practice that involves physical poses, concentration, and deep breathing. **Yoga nutrition** aims at cleansing, strengthening, and developing all levels of our human existence.\n",
    "Keeping that in mind, in the current web scrapping project, I would like to gather the data on **Yoga and Nutrition** from Google search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner image](https://i.imgur.com/CjeKM2H.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for a lot of things in the internet. These information are readily available but cannot be saved easily so we can use it later for any other purposes. One way is to copy the data manually and save it in your desktop. However, this is a very time consuming job. Web scraping is handy in such cases.\n",
    "\n",
    "**Web Scraping** is a technique used to automatically extract large amounts of data from websites and save it to a file or database. The data scraped will usually be in tabular or spreadsheet format(e.g : CSV file)\n",
    "\n",
    "![](https://i.imgur.com/m5lV5m9.png)\n",
    "\n",
    "Here, in this web scrapping we will scrap data from google results.\n",
    "\n",
    "We'll use the Python libraries requests and beautifulsoup4 to perform scrapping from the webpage.\n",
    "\n",
    "Here's an outline of the steps we'll follow:\n",
    "\n",
    "1. Download the webpage using requests.\n",
    "2. Parse the HTML source code using beautifulsoup4.\n",
    "3. Extract title, link and description.\n",
    "4. Compile the extracted information into Python lists and dictionaries.\n",
    "5. Extract and combine data from multiple pages.\n",
    "6. Save the extracted information to a CSV file.\n",
    "\n",
    "By the end of the project, we'll create a CSV file in the following format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/rqN5uDY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and importing the Libraries required for the current project.\n",
    "Before starting the project importing some of the dependencies is a good idea. The current project uses the following libraries:\n",
    "\n",
    "Requests - to download the the web page in the text format.\n",
    "BeautifulSoup - to be able to use the text downloaded from the Requests library to be used for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, python requests do not need headers and cookies. But in some situations when we request for the page content, we get a status code of 403 or 503. This means we cannot access the web page contents. In such cases we add headers and cookies to the argument of the requests.get() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {'User-agent':\n",
    "           'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "\n",
    "base_url = 'https://google.com/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Beautiful Soup to parse and extract information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the response from the particular URL using Requests.\n",
    "\n",
    "requests.get function from the requests library is used for downloading the webpage which is in the response object.\n",
    "In order to check if we can recieve the webpage as required for further processing, we use status_code property of response object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusable funtion to parse and extract information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url(url):\n",
    "    # Fetch the URL data using requests.get(url),store it in a variable, response.\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    return (response)    \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get Title, Link and Description of each article from html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title tag:h3 tag\n",
    "\n",
    "![](https://i.imgur.com/GA5FA78.png)\n",
    "\n",
    "Description tag: span\n",
    "\n",
    "![](https://i.imgur.com/fyrtl6c.png)\n",
    "\n",
    "Link tag: href tag\n",
    "\n",
    "![](https://i.imgur.com/i5PxmOd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(url):\n",
    "    response = get_url(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "     \n",
    "    articles_dict = []\n",
    "    for result in doc.select('.tF2Cxc'):\n",
    "        title = result.select_one('.DKV0Md').text\n",
    "        link = result.select_one('.yuRUbf a')['href']\n",
    "         \n",
    "# sometimes there's no description and we need to handle this exception\n",
    "        try: \n",
    "            desc = result.select_one('#rso .lyLwlc').text\n",
    "        except: desc = None\n",
    "       \n",
    "        articles_dict.append({\n",
    "           'Title': title,\n",
    "           'Link': link,\n",
    "           'Description': desc\n",
    "         })\n",
    "    return articles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting next page links from the tag 'a' and class = 'f1' and scraping data for first to ten pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Next page link tags information\n",
    "\n",
    "![](https://i.imgur.com/Zhs3rVz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_ten_pages(url):\n",
    "    response = get_url(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    Pagination_URL_tags = doc.find_all('a', class_ = \"fl\")\n",
    "    \n",
    "#Creating a list of Next_page_links by Concatenating base_url with tag['href']\n",
    "    Next_page_links = []\n",
    "    for tag in Pagination_URL_tags:\n",
    "        Next_page_links.append(base_url + tag['href'])\n",
    "\n",
    "#Scraping first page data\n",
    "    articles_dict = scrape_articles(url)\n",
    "\n",
    "#For each url scraping data.       \n",
    "    for url in Next_page_links:\n",
    "        dict1 = scrape_articles(url)\n",
    "        articles_dict.extend(dict1)\n",
    "\n",
    "#Dictionary to dataframe        \n",
    "    articles_df = pd.DataFrame.from_dict(articles_dict)\n",
    "    return articles_df   \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together in one block.\n",
    "Putting all the functions in one place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header\n",
    "headers = {'User-agent':\n",
    "           'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "\n",
    "base_url = 'https://google.com/'\n",
    "url = 'https://google.com/search?q='\n",
    "\n",
    "#Reusable funtion to parse and extract information.\n",
    "def get_url(url):\n",
    "    # Fetch the URL data using requests.get(url),store it in a variable, response.\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    return (response)\n",
    "\n",
    "#Function to get Title, Link and Description of each article from html tags\n",
    "def scrape_articles(url):\n",
    "    response = get_url(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "     \n",
    "    articles_dict = []\n",
    "    for result in doc.select('.tF2Cxc'):\n",
    "        title = result.select_one('.DKV0Md').text\n",
    "        link = result.select_one('.yuRUbf a')['href']\n",
    "         \n",
    "# sometimes there's no description and we need to handle this exception\n",
    "        try: \n",
    "            desc = result.select_one('#rso .lyLwlc').text\n",
    "        except: desc = None\n",
    "       \n",
    "        articles_dict.append({\n",
    "           'Title': title,\n",
    "           'Link': link,\n",
    "           'Description': desc\n",
    "         })\n",
    "    return articles_dict\n",
    "\n",
    "#Function to get Title, Link and Description of each article from html tags.\n",
    "def first_ten_pages(url):\n",
    "    response = get_url(url)\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    Pagination_URL_tags = doc.find_all('a', class_ = \"fl\")\n",
    "    \n",
    "#Creating a list of Next_page_links by Concatenating base_url with tag['href']\n",
    "    Next_page_links = []\n",
    "    for tag in Pagination_URL_tags:\n",
    "        Next_page_links.append(base_url + tag['href'])\n",
    "        \n",
    "#Scraping first page data\n",
    "    articles_dict = scrape_articles(url)\n",
    "    \n",
    "#For each url scraping data.       \n",
    "    for url in Next_page_links:\n",
    "        dict1 = scrape_articles(url)\n",
    "        articles_dict.extend(dict1)\n",
    "        \n",
    "#Ductionary to dataframe        \n",
    "    articles_df = pd.DataFrame.from_dict(articles_dict)\n",
    "    return articles_df   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting title, link and description for the topics \"Yoga\" and \"Yoga Nutrition\"(first 10 pages)\n",
    "\n",
    "Make two strings with default google search URL 'https://google.com/search?q=' and our customized search keyword.\n",
    "Concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topics\n",
    "topics= [\"Yoga\", \"Yoga Nutrition\"]\n",
    "url_list = []\n",
    "\n",
    "#Generating lsit of urls for Topics\n",
    "for text in topics:\n",
    "    url = 'https://google.com/search?q=' + text\n",
    "    url_list.append(url)\n",
    "url_list\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#Scraping data for topic urls and saving in dataframe. Merging two topics dataframes.\n",
    "for text in url_list:\n",
    "    Articles_df = first_ten_pages(text)\n",
    "    df = df.append(Articles_df)\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yoga - Wikipedia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoga</td>\n",
       "      <td>Yoga is a group of physical, mental, and spiri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoga for Everyone - The New York Times</td>\n",
       "      <td>https://www.nytimes.com/guides/well/beginner-yoga</td>\n",
       "      <td>Yoga 101. A set of specific exercises, called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yoga: What You Need To Know | NCCIH</td>\n",
       "      <td>https://www.nccih.nih.gov/health/yoga-what-you...</td>\n",
       "      <td>Yoga is an ancient and complex practice, roote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yoga: Methods, types, philosophy, and risks</td>\n",
       "      <td>https://www.medicalnewstoday.com/articles/286745</td>\n",
       "      <td>08-Jul-2021 — Yoga is an ancient practice that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All About Yoga: Poses, Types, Benefits, and More</td>\n",
       "      <td>https://www.everydayhealth.com/yoga/</td>\n",
       "      <td>Research has shown that yoga can help lower ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Total Nutrition - TriBalance Yoga Center</td>\n",
       "      <td>https://tribalance.com/total-nutrition-program/</td>\n",
       "      <td>What is the Tribalance Total Nutrition Program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>what is yogic diet - sattvik food | Lifestyle ...</td>\n",
       "      <td>https://www.lifestyleyogaworld.com/what-is-yog...</td>\n",
       "      <td>21-Jun-2021 — Our blog talks about the importa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>The Benefits of Yoga Put to The Test | Nutriti...</td>\n",
       "      <td>https://nutritionfacts.org/webinar/the-benefit...</td>\n",
       "      <td>Yoga is practiced by millions of Americans and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Dietry Principles | Kriyayoga Meditation</td>\n",
       "      <td>https://www.kriyayoga-yogisatyam.org/science-o...</td>\n",
       "      <td>The Science of Nutrition. When we prepare food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Yoga Nutrition - Yoga Earth</td>\n",
       "      <td>https://yogaearth.com/category/yoga-nutrition/</td>\n",
       "      <td>16-Dec-2022 — What to Eat Before and After Yog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                                     Yoga - Wikipedia   \n",
       "1               Yoga for Everyone - The New York Times   \n",
       "2                  Yoga: What You Need To Know | NCCIH   \n",
       "3          Yoga: Methods, types, philosophy, and risks   \n",
       "4     All About Yoga: Poses, Types, Benefits, and More   \n",
       "..                                                 ...   \n",
       "189           Total Nutrition - TriBalance Yoga Center   \n",
       "190  what is yogic diet - sattvik food | Lifestyle ...   \n",
       "191  The Benefits of Yoga Put to The Test | Nutriti...   \n",
       "192           Dietry Principles | Kriyayoga Meditation   \n",
       "193                        Yoga Nutrition - Yoga Earth   \n",
       "\n",
       "                                                  Link  \\\n",
       "0                   https://en.wikipedia.org/wiki/Yoga   \n",
       "1    https://www.nytimes.com/guides/well/beginner-yoga   \n",
       "2    https://www.nccih.nih.gov/health/yoga-what-you...   \n",
       "3     https://www.medicalnewstoday.com/articles/286745   \n",
       "4                 https://www.everydayhealth.com/yoga/   \n",
       "..                                                 ...   \n",
       "189    https://tribalance.com/total-nutrition-program/   \n",
       "190  https://www.lifestyleyogaworld.com/what-is-yog...   \n",
       "191  https://nutritionfacts.org/webinar/the-benefit...   \n",
       "192  https://www.kriyayoga-yogisatyam.org/science-o...   \n",
       "193     https://yogaearth.com/category/yoga-nutrition/   \n",
       "\n",
       "                                           Description  \n",
       "0    Yoga is a group of physical, mental, and spiri...  \n",
       "1    Yoga 101. A set of specific exercises, called ...  \n",
       "2    Yoga is an ancient and complex practice, roote...  \n",
       "3    08-Jul-2021 — Yoga is an ancient practice that...  \n",
       "4    Research has shown that yoga can help lower ph...  \n",
       "..                                                 ...  \n",
       "189  What is the Tribalance Total Nutrition Program...  \n",
       "190  21-Jun-2021 — Our blog talks about the importa...  \n",
       "191  Yoga is practiced by millions of Americans and...  \n",
       "192  The Science of Nutrition. When we prepare food...  \n",
       "193  16-Dec-2022 — What to Eat Before and After Yog...  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reseting index\n",
    "df = df.reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV file(s) with the extracted information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final dataframe to csv file.\n",
    "df.to_csv('Yoga_nutrition_articles.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this current project, the yoga and nutrition articles have been scrapped from first 10 google results pages.\n",
    "\n",
    "The following are the steps followed in this notebook.\n",
    "1. Downloaded the webpage using requests.\n",
    "2. Parsed the HTML source code using beautifulsoup4.\n",
    "3. Extracted Title,Link and Description details.\n",
    "4. Compiled the extracted information into Python lists and dictionaries.\n",
    "5. Extracted and combine data from multiple pages.\n",
    "6. Saved the extracted information to a CSV file.\n",
    "\n",
    "The CSV file we created has this format:\n",
    "![](https://i.imgur.com/iJrM3qz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can scrape the each article for more information like Written by, Reviewed by and Date.\n",
    "* We can get the information for other user defined topics and scrap of the data accordingly from google results.\n",
    "* We can use this data for NLP Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refrences\n",
    "1. https://jovian.ai/learn/zero-to-data-analyst-bootcamp/lesson/web-scraping-and-rest-apis\n",
    "\n",
    "2. https://practicaldatascience.co.uk/data-science/how-to-scrape-google-search-results-using-python\n",
    "\n",
    "3. https://medium.com/analytics-vidhya/web-scraping-amazon-reviews-a36bdb38b257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
